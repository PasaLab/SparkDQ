[spark]
spark.executor.instances = 20
spark.executor.cores = 4
spark.executor.memory = 4g
spark.yarn.am.memory = 1g
spark.yarn.queue = default
spark.pyspark.python = /home/experiment/anaconda3/bin/python

[hadoop]
java.home = /home/experiment/zhangyi-env/jdk1.8.0
hadoop.home = /home/experiment/zhangyi-env/hadoop-2.7.4
hadoop.conf.dir = /home/experiment/zhangyi-env/hadoop-2.7.4/etc/hadoop
hdfs.url = http://114.212.10.31:50070

[yarn]
yarn.conf.dir = /home/experiment/zhangyi-env/hadoop-2.7.4/etc/hadoop

[system]
app.name = SparkDQ
jars.name = dqlib-1.0-jar-with-dependencies.jar

[hbase]
hbase.host = "???"